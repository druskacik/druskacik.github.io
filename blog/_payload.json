[{"data":1,"prerenderedAt":884},["ShallowReactive",2],{"$sQPyZxu9EX":3},[4,333,574,804,842],{"id":5,"title":6,"body":7,"date":325,"description":326,"extension":327,"meta":328,"navigation":329,"path":330,"seo":331,"stem":332},"content/why-doesnt-openai-build-gpt-search-console.md","Why doesn't OpenAI build GPT Search Console?",{"type":8,"value":9,"toc":318},"minimal",[10,27,34,40,64,69,89,158,161,171,231,234,237,241,244,259,263,266,275],[11,12,13,14,18,19,26],"p",{},"More and more people are using an LLM instead of a traditional search engine. As a result, website owners can observe that an increasing share of their website audience is coming from an LLM chat, most often ",[15,16,17],"em",{},"chatgpt.com",". These users are also very relevant - famously, ",[20,21,25],"a",{"href":22,"rel":23},"https://vercel.com/blog/how-were-adapting-seo-for-llms-and-ai-search",[24],"nofollow","10% of new Vercel users come"," from ChatGPT.",[11,28,29],{},[30,31],"img",{"alt":32,"src":33},"Umami Analytics showing chatgpt.com as referrer","/umami-analytics.png",[11,35,36,37,39],{},"The problem is that while we can analyse and optimise for traditional search using Google Search Console, with LLMs we are blind. We see ",[15,38,17],{}," showing among top website referrers, but we have no clue what the conversations were about. We can only guess.",[11,41,42,47,48,47,53,47,58,63],{},[20,43,46],{"href":44,"rel":45},"https://mentionedby.ai",[24],"Many"," ",[20,49,52],{"href":50,"rel":51},"https://www.athenahq.ai/",[24],"services",[20,54,57],{"href":55,"rel":56},"https://arvow.com/llm-brand-mention-rank-tracker",[24],"popped",[20,59,62],{"href":60,"rel":61},"https://productrank.ai/",[24],"up",", promising to analyse mentions of brands across various LLMs, however, they mostly work by simulating user queries, and that approach can go only so far. It would be more useful to be able to see the context in conversations of real users, however, this is obviously difficult to do without breaching their privacy.",[65,66,68],"h2",{"id":67},"keyword-analysis-of-my-conversations","Keyword analysis of my conversations",[11,70,71,72,74,75,80,81,88],{},"Perhaps using some keyword extraction could provide relevant data without the need of sharing the whole conversation. I dumped my data from ",[15,73,17],{}," and analysed the links ChatGPT visited, together with ",[20,76,79],{"href":77,"rel":78},"https://github.com/MaartenGr/KeyBERT",[24],"KeyBERT"," to extract some relevant phrases from the conversations. Here are the results for domain ",[20,82,85],{"href":83,"rel":84},"https://nasa.gov",[24],[15,86,87],{},"nasa.gov",":",[90,91,92,108],"table",{},[93,94,95],"thead",{},[96,97,98,102,105],"tr",{},[99,100,101],"th",{},"domain",[99,103,104],{},"keyword",[99,106,107],{},"count",[109,110,111,122,131,140,149],"tbody",{},[96,112,113,116,119],{},[114,115,87],"td",{},[114,117,118],{},"current interstellar missions",[114,120,121],{},"9",[96,123,124,126,129],{},[114,125,87],{},[114,127,128],{},"future interstellar missions",[114,130,121],{},[96,132,133,135,138],{},[114,134,87],{},[114,136,137],{},"spacecraft solar interstellar",[114,139,121],{},[96,141,142,144,147],{},[114,143,87],{},[114,145,146],{},"future interstellar travel",[114,148,121],{},[96,150,151,153,156],{},[114,152,87],{},[114,154,155],{},"launched trajectories interstellar",[114,157,121],{},[11,159,160],{},"This was a conversation where I asked ChatGPT about the current projects in interstellar travel.",[11,162,163,164,88],{},"Another example, ",[20,165,168],{"href":166,"rel":167},"https://yelp.com",[24],[15,169,170],{},"yelp.com",[90,172,173,183],{},[93,174,175],{},[96,176,177,179,181],{},[99,178,101],{},[99,180,104],{},[99,182,107],{},[109,184,185,195,204,213,222],{},[96,186,187,189,192],{},[114,188,170],{},[114,190,191],{},"pizza prague venues",[114,193,194],{},"4",[96,196,197,199,202],{},[114,198,170],{},[114,200,201],{},"prague watch football",[114,203,194],{},[96,205,206,208,211],{},[114,207,170],{},[114,209,210],{},"pub location prague",[114,212,194],{},[96,214,215,217,220],{},[114,216,170],{},[114,218,219],{},"prague venues cater",[114,221,194],{},[96,223,224,226,229],{},[114,225,170],{},[114,227,228],{},"prague venues",[114,230,194],{},[11,232,233],{},"This was a conversation where I asked about some pubs in Prague where I could watch football.",[11,235,236],{},"Now imagine this scaled for millions of daily active users of ChatGPT. A product that would offer this data would be fire.",[65,238,240],{"id":239},"why-dont-they-build-this","Why don't they build this?",[11,242,243],{},"This definitely isn't an original idea (see the projects linked above), but I believe noone executed it well enough yet. Simulating user queries can't compete with the data companies like OpenAI already have, and they probably have better things to do. Also, many users would have legititimate concerns if they knew their data was used like this, and they could switch to another LLM provider.",[11,245,246,247,258],{},"However, I am sure the next few years",[248,249,250],"sup",{},[20,251,257],{"href":252,"ariaDescribedBy":253,"dataFootnoteRef":255,"id":256},"#user-content-fn-1",[254],"footnote-label","","user-content-fnref-1","1"," will see a rise of product like this, or something equivalent (think, for example, ChatGPT Trends similar to Google Trends?). The data is just too useful for the companies not to extract some additional value out of it.",[65,260,262],{"id":261},"some-ideas","Some ideas",[11,264,265],{},"As I said, some companies are trying to get this data by simulating user queries. What if we used real queries instead (posts from Reddit, Quora, etc.)? We could run this daily, see which websites appear often in the GPT, and even contact them, saying \"look, your website comes up in GPT from real users' queries ... wanna buy the data?\". Does anyone do this?",[11,267,268,269,274],{},"Also, would it be possible to capture the keywords of the conversation and add it to the query, similarly to the ",[20,270,273],{"href":271,"rel":272},"https://en.wikipedia.org/wiki/UTM_parameters",[24],"UTM parameters","? Maybe some companies would be willing to pay for this.",[276,277,280,285],"section",{"className":278,"dataFootnotes":255},[279],"footnotes",[65,281,284],{"className":282,"id":254},[283],"sr-only","Footnotes",[286,287,288],"ol",{},[289,290,292,293,298,299,304,305,310,311],"li",{"id":291},"user-content-fn-1","Perhaps even sooner. Two years ago at the ",[20,294,297],{"href":295,"rel":296},"https://www.eeml.eu/",[24],"EEML"," event my ",[20,300,303],{"href":301,"rel":302},"https://www.youtube.com/live/Sg4kUSvl7qI?si=JgxSOl-HJoBPbqNQ&t=18560",[24],"final project"," was about personalization of ChatGPT using custom prompt \"learned\" from previous conversations. A few weeks after that, OpenAI introduced ",[20,306,309],{"href":307,"rel":308},"https://openai.com/index/custom-instructions-for-chatgpt/",[24],"custom instructions",". Sam, I know you are a big fan of me. ",[20,312,317],{"href":313,"ariaLabel":314,"className":315,"dataFootnoteBackref":255},"#user-content-fnref-1","Back to reference 1",[316],"data-footnote-backref","↩",{"title":255,"searchDepth":319,"depth":319,"links":320},2,[321,322,323,324],{"id":67,"depth":319,"text":68},{"id":239,"depth":319,"text":240},{"id":261,"depth":319,"text":262},{"id":254,"depth":319,"text":284},"2025-06-21","More and more people are using an LLM instead of a traditional search engine. As a result, website owners can observe that an increasing share of their website audience is coming from an LLM chat, most often chatgpt.com. These users are also very relevant - famously, 10% of new Vercel users come from ChatGPT.","md",{},true,"/why-doesnt-openai-build-gpt-search-console",{"title":6,"description":326},"why-doesnt-openai-build-gpt-search-console",{"id":334,"title":335,"body":336,"date":569,"description":340,"extension":327,"meta":570,"navigation":329,"path":571,"seo":572,"stem":573},"content/chat-with-llm-from-address-bar-in-firefox.md","Direct chat with LLM from address bar in Firefox",{"type":8,"value":337,"toc":565},[338,341,372,375,378,382,396,411,414,418,427,443,449,474,479,511,518,524,533,551],[11,339,340],{},"It's no secret that LLMs have largely replaced search engines in the last few years. However, using them as quickly as a search engine has remained a bit cumbersome for me. My workflow has been as follows:",[286,342,343,346,349,369],{},[289,344,345],{},"if not in browser, navigate to it using Alt + Tab shortcut",[289,347,348],{},"new tab with Ctrl + T",[289,350,351,352,356,357,362,363,368],{},"navigate to LLM website of choice - this was ",[20,353,17],{"href":354,"rel":355},"https://chatgpt.com/",[24]," since its launch till Claude 3.5 Sonnet, ",[20,358,361],{"href":359,"rel":360},"https://claude.ai/",[24],"claude.ai"," till Gemini 2.5 Pro, and ",[20,364,367],{"href":365,"rel":366},"https://gemini.google.com/",[24],"gemini.google.com"," till now.",[289,370,371],{},"write and run query",[11,373,374],{},"Not a terrible workflow, but it's one step longer than the traditional search workflow that omits the step 3. Moreover, if the internet speed is lagging, waiting for the LLM website to load can take a few awkward seconds. This can be very annoying, since I may have to wait twice - for the web to load, and for the LLM to answer. In the traditional search workflow, I wait only once (okay, maybe more if I open any of the results, but at least I receive some response after only one wait period.)",[11,376,377],{},"Luckily, this workflow can be optimalised.",[65,379,381],{"id":380},"url-to-chat","Url to chat",[11,383,384,385,390,391,395],{},"Some LLM provider support starting a chat directly from url, e.g. visiting ",[20,386,389],{"href":387,"rel":388},"https://chat.mistral.ai/chat?q=ubuntu%20unrar%20file",[24],"https://chat.mistral.ai/chat?q=ubuntu unrar file"," opens Mistral's Le Chat and starts a chat with user's message ",[392,393,394],"code",{},"ubuntu unrar file",". The model starts responding immediately, no need to press additional Enter.",[11,397,398,399,404,405,410],{},"This is supported also in ChatGPT ( ",[20,400,403],{"href":401,"rel":402},"https://chatgpt.com/?q=ubuntu%20unrar%20file",[24],"https://chatgpt.com/?q=ubuntu unrar file"," ) and Claude ( ",[20,406,409],{"href":407,"rel":408},"https://claude.ai/new?q=ubuntu%20unrar%20file",[24],"https://claude.ai/new?q=ubuntu unrar file"," ), however, with Claude the chat is only pre-filled and it's necessary to send the message manually. I tried to find similar functionality in Gemini, but without success.",[11,412,413],{},"Since Firefox supports adding custom search engines, we can use this to access the LLMs directly from the address bar!",[65,415,417],{"id":416},"custom-search-engine","Custom search engine",[11,419,420,421,426],{},"To add a custom search engine, we need to follow these steps (",[20,422,425],{"href":423,"rel":424},"https://superuser.com/a/1756774",[24],"source","):",[286,428,429],{},[289,430,431,432,435,436,439,440],{},"go to ",[392,433,434],{},"about:config"," and set property ",[392,437,438],{},"browser.urlbar.update2.engineAliasRefresh"," to ",[392,441,442],{},"true",[11,444,445],{},[30,446],{"alt":447,"src":448},"about:config screenshot","/about-config.png",[286,450,451,463],{"start":319},[289,452,453,454,457,458,462],{},"in ",[392,455,456],{},"about:preferences#search"," there appears a new ",[459,460,461],"strong",{},"Add"," button - we can use it to add a new search engine",[289,464,465,466,469,470,473],{},"to add e.g. Mistral as a search engine, we use url ",[392,467,468],{},"https://chat.mistral.ai/chat?q=%s"," - here ",[392,471,472],{},"%s"," represents the search term",[11,475,476],{},[30,477],{"alt":447,"src":478},"/add-new-search-engine.png",[11,480,481,482,485,486,489,490,493,494,497,498,503,504,507,508,510],{},"As an alias, I like to use simply one letter, in this case ",[392,483,484],{},"m",". The more common way is to use an alias like ",[392,487,488],{},"@m"," or ",[392,491,492],{},"!m"," (",[392,495,496],{},"!"," is used in ",[20,499,502],{"href":500,"rel":501},"https://duckduckgo.com/bangs",[24],"DuckDuckGo bangs","), however, writing a special character can be time-consuming when in flow. I use this convention also with search engines, e.g. writing ",[392,505,506],{},"g ubuntu unrar file"," searches the query ",[392,509,394],{}," on Google instead of my default search engine. The caveat is that the queries that start with a single letter will be trimmed and sent to a different search than intended, however, I think this happened to me only a handful of times during the years I've been using the one letter alias convention.",[11,512,513,514,517],{},"Now, when we type ",[392,515,516],{},"m ubuntu unrar file"," in the address bar, we are sent directly to new chat and immediately receive a response.",[11,519,520],{},[30,521],{"alt":522,"src":523},"Chat with Mistral","/le-chat.png",[11,525,526,527,532],{},"I chose Mistral as the example because its ",[20,528,531],{"href":529,"rel":530},"https://help.mistral.ai/en/articles/268659-flash-answers",[24],"flash answers"," are perfect for these simple queries that I know have a simple answer that I'm lazy or ignorant to know by myself. Also, Mistral seems to work even without being logged in.",[11,534,535,536,539,540,543,544,547,548,550],{},"This setup works also for at least ChatGPT (format url ",[392,537,538],{},"https://chatgpt.com/?q=%s",") and Claude (format url ",[392,541,542],{},"https://claude.ai/new?q=%s","). The one letter alias convention is a bit problematic here not only because both the models' names start with ",[392,545,546],{},"c",", but also searching for queries related to C programming language may get broken if not careful. But since I don't work with C, I am not afraid to use ",[392,549,546],{}," as the alias for ChatGPT. The C programmers among us will have to think of something else.",[11,552,553,554,489,559,564],{},"Finally, there probably exists a simpler option than all of this - to use the built-in shortcuts in ",[20,555,558],{"href":556,"rel":557},"https://claude.ai/download",[24],"Claude Desktop",[20,560,563],{"href":561,"rel":562},"https://openai.com/chatgpt/desktop/",[24],"ChatGPT Desktop",". But since I prefer easy interchange between searching with search engines and quick chat with an LLM, the address bar way is the one I prefer. In theory, the brief moment between the realisation I need to search and typing the first letter in the address bar gives my mind enough time to decide which option will be the fastest to get to the result I need.",{"title":255,"searchDepth":319,"depth":319,"links":566},[567,568],{"id":380,"depth":319,"text":381},{"id":416,"depth":319,"text":417},"2025-04-29",{},"/chat-with-llm-from-address-bar-in-firefox",{"title":335,"description":340},"chat-with-llm-from-address-bar-in-firefox",{"id":575,"title":576,"body":577,"date":798,"description":799,"extension":327,"meta":800,"navigation":329,"path":801,"seo":802,"stem":803},"content/claude-as-a-video-editor.md","Claude as a video editor",{"type":8,"value":578,"toc":795},[579,588,591,602,611,620,629,771,774,791],[11,580,581,582,587],{},"Recently I wanted to speed up a demo video 10 times. Having minimum experience with video editing but convinced that such a simple action should be possible with a built-in Windows editor, I mindlessly searched \"windows video speed up\" on DuckDuckGo and clicked the first credible-looking result. According to the ",[20,583,586],{"href":584,"rel":585},"https://www.thewindowsclub.com/how-to-speed-up-a-video-in-windows",[24],"tutorial",", the speed-up could be done in the native Photos app on Windows. Great!",[11,589,590],{},"Following the tutorial, I",[286,592,593,596,599],{},[289,594,595],{},"launched the Photos app",[289,597,598],{},"imported and selected the video",[289,600,601],{},"created a new video project – wait, how do I create a new video project?",[11,603,604,605,610],{},"Scrolling below in the tutorial, there was a screenshot with a red box (I just love those) directing me to simply click the “New” button after selecting a video. Easy. Unfortunately, for some mysterious reason I could not find the button in the app, and after a minute of clicking around I hopelessly thought: What do I do now? Google",[248,606,607],{},[20,608,257],{"href":252,"ariaDescribedBy":609,"dataFootnoteRef":255,"id":256},[254]," “windows photos new video project”? Download some shady app the tutorial definitely recommends as an alternative option?",[11,612,613,614,619],{},"Finally, I thought: wait, I'm a programmer. I have zero interest in learning video editing, I just want to speed up a stupid demo video for my ",[20,615,618],{"href":616,"rel":617},"https://illuminovel.com",[24],"half-baked project",". There's definitely a way to achieve this with python. And what better way to write a simple python script than instructing an LLM using a natural language?",[11,621,622,623,628],{},"I greeted ",[20,624,627],{"href":625,"rel":626},"https://claude.ai",[24],"Claude"," with a generic “Write a python script to speed up an mp4 video 10x.”, blindly copied the code, rewrote the input and output path, pip-installed a required library, runned the script, and voila, it worked liked charm! No need to navigate confusing Windows UI or install any apps I probably wouldn’t use. For reference, here’s the script:",[630,631,638],"pre",{"className":632,"code":633,"filename":634,"highlights":635,"language":636,"meta":637,"style":255},"language-python shiki shiki-themes github-light github-dark","from moviepy.editor import VideoFileClip  \n\ndef speed_up_video(input_path, output_path, speed_factor=10):\n    # Load the video clip\n    video = VideoFileClip(input_path)\n    \n    # Speed up the video\n    fast_video = video.speedx(speed_factor)\n    \n    # Write the result to a file\n    fast_video.write_videofile(output_path)\n    \n    # Close the video clips\n    video.close()\n    fast_video.close()\n\n# Example usage\ninput_video = \"input_video.mp4\"\noutput_video = \"output_video_10x.mp4\"\nspeed_up_video(input_video, output_video)\n\nprint(f\"Video has been sped up 10x and saved as {output_video}\")\n","file.py",[319],"python","meta-info=val",[392,639,640,648,655,661,667,673,679,685,691,696,702,708,713,719,725,731,736,742,748,754,760,765],{"__ignoreMap":255},[641,642,645],"span",{"class":643,"line":644},"line",1,[641,646,647],{},"from moviepy.editor import VideoFileClip  \n",[641,649,652],{"class":650,"line":319},[643,651],"highlight",[641,653,654],{"emptyLinePlaceholder":329},"\n",[641,656,658],{"class":643,"line":657},3,[641,659,660],{},"def speed_up_video(input_path, output_path, speed_factor=10):\n",[641,662,664],{"class":643,"line":663},4,[641,665,666],{},"    # Load the video clip\n",[641,668,670],{"class":643,"line":669},5,[641,671,672],{},"    video = VideoFileClip(input_path)\n",[641,674,676],{"class":643,"line":675},6,[641,677,678],{},"    \n",[641,680,682],{"class":643,"line":681},7,[641,683,684],{},"    # Speed up the video\n",[641,686,688],{"class":643,"line":687},8,[641,689,690],{},"    fast_video = video.speedx(speed_factor)\n",[641,692,694],{"class":643,"line":693},9,[641,695,678],{},[641,697,699],{"class":643,"line":698},10,[641,700,701],{},"    # Write the result to a file\n",[641,703,705],{"class":643,"line":704},11,[641,706,707],{},"    fast_video.write_videofile(output_path)\n",[641,709,711],{"class":643,"line":710},12,[641,712,678],{},[641,714,716],{"class":643,"line":715},13,[641,717,718],{},"    # Close the video clips\n",[641,720,722],{"class":643,"line":721},14,[641,723,724],{},"    video.close()\n",[641,726,728],{"class":643,"line":727},15,[641,729,730],{},"    fast_video.close()\n",[641,732,734],{"class":643,"line":733},16,[641,735,654],{"emptyLinePlaceholder":329},[641,737,739],{"class":643,"line":738},17,[641,740,741],{},"# Example usage\n",[641,743,745],{"class":643,"line":744},18,[641,746,747],{},"input_video = \"input_video.mp4\"\n",[641,749,751],{"class":643,"line":750},19,[641,752,753],{},"output_video = \"output_video_10x.mp4\"\n",[641,755,757],{"class":643,"line":756},20,[641,758,759],{},"speed_up_video(input_video, output_video)\n",[641,761,763],{"class":643,"line":762},21,[641,764,654],{"emptyLinePlaceholder":329},[641,766,768],{"class":643,"line":767},22,[641,769,770],{},"print(f\"Video has been sped up 10x and saved as {output_video}\")\n",[11,772,773],{},"This is just a simple example, but more and more in the recent two years have I found myself turning to LLMs as the first solution. True, usually for simple tasks such as this one, but with recent focus on agentic workflows, I think it’s safe to say the capabilities will be only more powerful. Specialized software tools will always be necessary for more fine-grained control of the output, but more and more often we’ll be able to get the result only by explaining it to an AI. Great time to be alive!",[276,775,777,780],{"className":776,"dataFootnotes":255},[279],[65,778,284],{"className":779,"id":254},[283],[286,781,782],{},[289,783,784,787,788],{"id":291},[15,785,786],{},"google"," is such a practical word that I use it even when I don't actually use the Google search engine ",[20,789,317],{"href":313,"ariaLabel":314,"className":790,"dataFootnoteBackref":255},[316],[792,793,794],"style",{},"html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}",{"title":255,"searchDepth":319,"depth":319,"links":796},[797],{"id":254,"depth":319,"text":284},"2025-02-20","Recently I wanted to speed up a demo video 10 times. Having minimum experience with video editing but convinced that such a simple action should be possible with a built-in Windows editor, I mindlessly searched \"windows video speed up\" on DuckDuckGo and clicked the first credible-looking result. According to the tutorial, the speed-up could be done in the native Photos app on Windows. Great!",{},"/claude-as-a-video-editor",{"title":576,"description":799},"claude-as-a-video-editor",{"id":805,"title":806,"body":807,"date":836,"description":837,"extension":327,"meta":838,"navigation":329,"path":839,"seo":840,"stem":841},"content/philip-glass-88-years.md","Philip Glass: 88 years",{"type":8,"value":808,"toc":834},[809,816,819,825,828],[11,810,811,812,815],{},"Philip Glass celebrated 88 years this January, and he's still composing music. The ",[15,813,814],{},"Prelude for organ"," premiered on his 31 January birthday is a short, but powerful piece of music. What an inspiration, 88 and still contributing to the world!",[11,817,818],{},"FB Post:",[11,820,821],{},[20,822,823],{"href":823,"rel":824},"https://www.facebook.com/philipglassmusic/posts/pfbid0DqzL8Le8MwrQn3Qv4LVRAF3TmrRgQGmLQbijEjanh99P27f5brciKvabS8MvZWp8l",[24],[11,826,827],{},"Piece on Spotify:",[11,829,830],{},[20,831,832],{"href":832,"rel":833},"https://open.spotify.com/track/3KB4Mvn2VgVstU0rONXydS?si=c18232874abf48f6",[24],{"title":255,"searchDepth":319,"depth":319,"links":835},[],"2025-02-01","Philip Glass celebrated 88 years this January, and he's still composing music. The Prelude for organ premiered on his 31 January birthday is a short, but powerful piece of music. What an inspiration, 88 and still contributing to the world!",{},"/philip-glass-88-years",{"title":806,"description":837},"philip-glass-88-years",{"id":843,"title":844,"body":845,"date":877,"description":878,"extension":327,"meta":879,"navigation":329,"path":880,"seo":881,"stem":883},"content/the-difference-of-celestial-bodies.md","The Difference of Celestial Bodies (poem)",{"type":8,"value":846,"toc":875},[847,857,863],[11,848,849,850,853,854,856],{},"A planet orbits around a star",[851,852],"br",{},"\nThat's me",[851,855],{},"\nYou are the star",[11,858,859,860,862],{},"The planet disappears and nothing happens to the star",[851,861],{},"\nThat's like when I fade away",[11,864,865,866,868,869,871,872,874],{},"But when the star disappears,",[851,867],{},"\nthe planet stumbles and remains alone,",[851,870],{},"\nlost in this vast boundless world",[851,873],{},"\nThat's like when you fade away",{"title":255,"searchDepth":319,"depth":319,"links":876},[],"2024-01-12","A planet orbits around a star\\nThat's me\\nYou are the star",{},"/the-difference-of-celestial-bodies",{"title":844,"description":882},"A planet orbits around a star\nThat's me\nYou are the star","the-difference-of-celestial-bodies",1750501124248]